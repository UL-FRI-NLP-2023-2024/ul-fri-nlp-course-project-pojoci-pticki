{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import metrics\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(611, 2)\n",
      "(38, 2)\n",
      "(194, 2)\n",
      "(90, 2)\n",
      "(228, 2)\n",
      "(99, 2)\n",
      "(9, 2)\n",
      "(82, 2)\n",
      "(23, 2)\n"
     ]
    }
   ],
   "source": [
    "# Branje vseh fajlov\n",
    "\n",
    "df1 = pd.read_excel('../data/CollabWriteAnalysisCountCodesLadyorTigerF20nS21S22wGSAnalysis27Jan2023_14Mar2024CleanF.xlsm', usecols=[\"Message\", \"R2DiscussionType\"])\n",
    "df2 = pd.read_excel('../data/s20 chat.xlsm', usecols=[\"Message\", \"R2 Discussion Type\"])\n",
    "df3 = pd.read_excel('../data/s20 crew.xlsm', usecols=[\"Message\", \"R1 Discussion type\"])\n",
    "df4 = pd.read_excel('../data/s21 chat.xlsm', usecols=[\"Message\", \"R2 Discussion Type\"])\n",
    "df5 = pd.read_excel('../data/s21 crew.xlsm', usecols=[\"Message\", \"R2 Discussion Type\"])\n",
    "df6 = pd.read_excel('../data/s22 chat sync.xlsm', usecols=[\"Message\", \"PG: Discussion (R2)\"])\n",
    "df7 = pd.read_excel('../data/s22 chat async.xlsm', usecols=[\"Message\", \"PG: Discussion (R2)\"])\n",
    "df8 = pd.read_excel('../data/s22 crew sync.xlsm', usecols=[\"Message\", \"PG: Discussion\"])\n",
    "df9 = pd.read_excel('../data/s22 crew async.xlsm', usecols=[\"Message\", \"PG: Discussion\"])\n",
    "\n",
    "print(df1.shape)\n",
    "print(df2.shape)\n",
    "print(df3.shape)\n",
    "print(df4.shape)\n",
    "print(df5.shape)\n",
    "print(df6.shape)\n",
    "print(df7.shape)\n",
    "print(df8.shape)\n",
    "print(df9.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n"
     ]
    }
   ],
   "source": [
    "# Zdru≈æevanje vseh data frejmov v enega\n",
    "df1.columns = [\"Message\", \"Discussion Type\"]\n",
    "df2.columns = [\"Message\", \"Discussion Type\"]\n",
    "df3.columns = [\"Message\", \"Discussion Type\"]\n",
    "df4.columns = [\"Message\", \"Discussion Type\"]\n",
    "df5.columns = [\"Message\", \"Discussion Type\"]\n",
    "df6.columns = [\"Message\", \"Discussion Type\"]\n",
    "df7.columns = [\"Message\", \"Discussion Type\"]\n",
    "df8.columns = [\"Message\", \"Discussion Type\"]\n",
    "df9.columns = [\"Message\", \"Discussion Type\"]\n",
    "\n",
    "\n",
    "frames = [df1, df2, df3, df4, df5, df6, df7, df8, df9]\n",
    "df = pd.concat(frames, ignore_index=True)\n",
    "\n",
    "print(df[\"Discussion Type\"].isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 2)\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.dropna(subset=['Discussion Type'])\n",
    "df.drop_duplicates(subset=[\"Message\"], inplace=True)\n",
    "print(df.shape)\n",
    "print(df[\"Discussion Type\"].isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion Type\n",
      "Seminar                  371\n",
      "Deliberation             101\n",
      "Social                    91\n",
      "UX                        62\n",
      "Procedure                 55\n",
      "Imaginative entry         16\n",
      "Other                      6\n",
      "Imaginative Entry          3\n",
      "Seminar, Deliberation      2\n",
      "Imaginative                2\n",
      "Social, Deliberation       1\n",
      "Deliberation, Seminar      1\n",
      "Social, Procedure          1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(df[\"Discussion Type\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discussion Type\n",
      "Seminar              371\n",
      "Deliberation         101\n",
      "Social                91\n",
      "UX                    62\n",
      "Procedure             55\n",
      "Imaginative Entry     21\n",
      "Others                 5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Mapping similar Discussion Types\n",
    "discussion_type_mapping = {\n",
    "    'Social': 'Social',\n",
    "    'social': 'Social',\n",
    "    'Deliberation': 'Deliberation',\n",
    "    'Deiberation': 'Deliberation',\n",
    "    'Seminar': 'Seminar',\n",
    "    'Procedure': 'Procedure',\n",
    "    'procedure': 'Procedure',\n",
    "    'UX': 'UX',\n",
    "    'UX/UI': 'UX',\n",
    "    'Imaginative entry': 'Imaginative Entry',\n",
    "    'Imaginative Entry': 'Imaginative Entry',\n",
    "    'Imaginative': 'Imaginative Entry',\n",
    "    'Seminar, Deliberation': 'Others',\n",
    "    'Social, Procedure': 'Others',\n",
    "    'Social, Deliberation': 'Others',\n",
    "    'Deliberation, Seminar': 'Others'\n",
    "}\n",
    "\n",
    "df['Discussion Type'] = df['Discussion Type'].map(discussion_type_mapping)\n",
    "discussion_type_counts = df['Discussion Type'].value_counts()\n",
    "discussion_type_counts['Others'] = discussion_type_counts[discussion_type_counts < 20].sum()\n",
    "# discussion_type_counts.drop(discussion_type_counts[discussion_type_counts < 20].index, inplace=True)\n",
    "print(discussion_type_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharni nov data frejm v csv fajl\n",
    "df.to_csv('../data/final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.dropna(subset=['Message'])\n",
    "# print(df.shape) # all messages have values\n",
    "# filtered_df1 = df[['Message', 'R2DiscussionType']]\n",
    "# filtered_df2 = df[['Message', 'R2DiscussionTypeInterpNothers']]\n",
    "# filtered_df3 = df[['Message', 'CollapsR2DiscussionTypeInterpNothers']]\n",
    "# filtered_df4 = df[['Message', 'R2DialogicSpell']]\n",
    "# filtered_df5 = df[['Message', 'BinaryR2DialogicSpell']]\n",
    "# filtered_df6 = df[['Message', 'R2Uptake']]\n",
    "# filtered_df7 = df[['Message', 'BinaryR2Uptake']]\n",
    "\n",
    "# # other ways to handle missing labels?\n",
    "# filtered_df4 = filtered_df4.dropna(subset=['R2DialogicSpell'])\n",
    "# filtered_df5 = filtered_df5.dropna(subset=['BinaryR2DialogicSpell'])\n",
    "# filtered_df6 = filtered_df6.dropna(subset=['R2Uptake'])\n",
    "# a=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specify which target to model (filtered1/2/3/.../7)\n",
    "# dataframe = filtered_df3\n",
    "\n",
    "# vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "# X = vectorizer.fit_transform(dataframe['Message'].values)\n",
    "# y = dataframe.iloc[:, 1].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(y_train)\n",
    "# y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# model = xgb.XGBClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# print(\"Classification report:\")\n",
    "# print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # specify which target to model (filtered1/2/3/.../7)\n",
    "# dataframe = filtered_df3\n",
    "\n",
    "# count_vectorizer = CountVectorizer()\n",
    "# X = count_vectorizer.fit_transform(dataframe['Message'].values)\n",
    "# y = dataframe.iloc[:, 1].values\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# label_encoder = LabelEncoder()\n",
    "# y_train = label_encoder.fit_transform(y_train)\n",
    "# y_test = label_encoder.fit_transform(y_test)\n",
    "\n",
    "# model = xgb.XGBClassifier()\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "\n",
    "# print(\"Classification report:\")\n",
    "# print(metrics.classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
